Whether we are opera singers or shower-wailers, ballet dancers or awkward shufflers, we all understand how music makes us feel, and more importantly, makes us move. Moving to music is so much a part of the human experience that it seems innate to us as a species. A recent study supports this, showing that fetuses react to music with increased motion, and in some cases, open their mouths as if to sing. Once out of the womb, this response only grows: a catchy tune makes hips swing and toes tap, and in certain situations, heads bang.

The music that moves us is itself a product of movement. As a musician who is a tactile learner, I’m keenly aware of the way a piece feels as I play it. Despite years of piano teachers telling me to read the page in front of me while I play, my eyes habitually wander to my hands, where the music is really happening. This gap between reading and performing music keeps me from fully expressing my musical ideas.

As a way to bridge this divide, I am trying to create a simple instrument that translates movement directly into music, using motion to capture melodic ideas and expressions. I got this idea while watching a lively orchestra conductor, who sometimes overshadowed the players so much that he seemed to be dancing alone, pulling notes through the air with his baton. Enchanted by how effortlessly he stirred the ocean of sound around him, I caught myself swishing my hands back and forth to the beat. As I lifted my arm to match the swelling tempo, I wondered: what if we could turn all kinds of movement into melodies?

It occurred to me that I could apply my skills in computer science and digital media to create a movement-to music application. To a computer everything is math, including music and movement. Every note and motion can be tracked, stored, and broken down into a set of variables, based on information from an outside source, such as a computer mouse or touchpad. I am currently taking advantage of this relationship by creating a web-based application that synthesizes music based on interactions with the cursor. The program, once completed, will play notes as the mouse is pressed, with unique pitch and tone determined by the position and motion of the pointer.

Eventually, I’d like to take this concept further using more sophisticated technology. I plan to take data from a motion sensor or camera and convert it directly into sound, using a simple device that tracks movement and translates its vertical position into musical pitch, its horizontal position into musical dynamics (soft to loud), and its speed into musical tone. Imagine being able to move your hand to generate a pitch that changes with the direction of movement, producing a musical phrase. Sophisticated users would be able to control relationships between variables to suit their needs; for example, they could link various components of movement (such as direction or speed in all three dimensions) to a wide range of musical characteristics, including, but not limited to, timbre, harmonics, and distortion.

Ultimately, artists could use my instrument to make music from anything that moves: dancers onstage, migrating birds, traffic at a busy intersection. It would not only close the gap between the conception and realization of music, but it could open new creative pathways that combine music and motion. As for me, I look forward to performing on an empty stage, directing an invisible orchestra with the flick of my wrist.